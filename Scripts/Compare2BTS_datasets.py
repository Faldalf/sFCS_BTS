# Compare2BTS_datasets.py
# Script to statistically compare two BTS datasets. Output of the script is a p-vlaue to inform on rejection of the null hypothesis that both datasets were sampled from the same underlying distribution. If p < 0.01 it is likely that the data originate from two different underlying distributions, ie., the data are significantly different.
# This is a cleaned-up version to accompany the BTS histogram manuscript.
#
# sFCS fitting parameters (as for example generated by FoCuS_scan) are required to be .xlsx or .csv files as input. 
# The columns should be named 'txy1' and 'cpm (kHz)'
#
# User needs to define the sample names/conditions in the list "samples"
# User can adapt plotting and histogram parameters as appropriate (ie change binning, max values etc). 
# Falk Schneider @faldalf 31/01/2024
#
# Questions and feedback are welcome

#%% Improts
import numpy as np
import os
import matplotlib.pyplot as plt
import pandas as pd
from tkinter import filedialog
from tkinter import *
from sklearn.utils import shuffle

#%% Functions
def make_hist (data, binning, x_min, txy_max, cpm_max):
    histo0 = np.histogram2d (np.log (data[0]), data[1], bins = binning, range = [[x_min, txy_max],[0, cpm_max]], density = True) 
    return histo0

def compute_test_statistic(hist1, hist2):
#(Bhattacharyya distance)
    hist1_normalized = hist1 / np.sum(hist1)
    hist2_normalized = hist2 / np.sum(hist2)
    sqrt_product = np.sqrt(hist1_normalized * hist2_normalized)
    return -np.log(np.sum(sqrt_product))

def pad_histograms (histo1, histo2, pad = 1e-9):
    # we need to add a small constant to not screw up (padding)
    
    hist1 = histo1[0] + pad * np.random.rand(*histo1[0].shape)
    hist2 = histo2[0] + pad * np.random.rand(*histo2[0].shape)
    return hist1, hist2

def make_figure_2hist (data_txy1, data_cpm1, data_txy2, data_cpm2, binning, x_min, txy_max):
    fig1, axes = plt.subplots(nrows=1, ncols=2,  figsize=(12, 4)) 

    for i, ax in enumerate(axes.flatten()):
        ax.set_xlabel ('Ln (Transit time (ms)/(ms))', fontsize = 14)
        ax.set_ylabel ('Brightness (kHz)', fontsize = 14)
        ax.tick_params (axis='x', labelsize = 14)
        ax.tick_params (axis='y', labelsize = 14)     

    histo1 = axes [0].hist2d (np.log(data_txy1), data_cpm1, bins = binning, range = [[x_min, txy_max],[0, cpm_max]], density = True) 
    histo2 = axes [1].hist2d (np.log(data_txy2), data_cpm2, bins = binning, range = [[x_min, txy_max],[0, cpm_max]], density = True) 
    plt.tight_layout ()
    return fig1, axes

def generate_null_statistics (data_txy1, data_cpm1, data_txy2, data_cpm2, sub_sampling = 0.2, n_iterations = 10000):
    #%This is the significance test 
    null_statistics = []
    for _ in range(n_iterations):
        combined_txy = np.concatenate ([data_txy1, data_txy2])
        combined_cpm = np.concatenate ([data_cpm1, data_cpm2])
        
        # Shuffling the data together in uniso
        combined_txy_shuffle, combined_cpm_shuffle = shuffle (combined_txy,combined_cpm)
        
        
        data_txy1_shuffle = combined_txy_shuffle[:int(len(data_txy1)*sub_sampling)]
        data_txy2_shuffle = combined_txy_shuffle[-int(len(data_txy2)*sub_sampling):]
        
        data_cpm1_shuffle = combined_cpm_shuffle [:int(len (data_cpm1)*sub_sampling)]
        data_cpm2_shuffle = combined_cpm_shuffle [-int (len (data_cpm2)*sub_sampling):]
        
       
        hist1_shuffled = make_hist ([data_txy1_shuffle, data_cpm1_shuffle], binning, x_min, txy_max, cpm_max)
        hist2_shuffled = make_hist ([data_txy2_shuffle, data_cpm2_shuffle], binning, x_min, txy_max, cpm_max)
        
        hist1_shuff_pad, hist2_shuff_pad = pad_histograms (hist1_shuffled, hist2_shuffled, pad = 1e-9)
        
        # Compute test statistic on permuted histograms
        permuted_statistic = compute_test_statistic(hist1_shuff_pad, hist2_shuff_pad)
        null_statistics.append(permuted_statistic)
    return null_statistics
    
def load_sFCS_data (samples): 
# Small helper function to read in sFCS fittingparameters from FoCuS_scan or simulations    

    root = Tk()
    root.filename =  filedialog.askopenfilename(title = ["Load file to analyse: "+ samples],filetypes = (("FCS Fittingparameters",".csv .xlsx"),("all files","*.*")))
    filename = root.filename
    filename = filename.replace("/","\\\\")
    root.withdraw()

    if 'xlsx' in filename:
        data = pd.read_excel (filename)
        data = data.drop(data.index[-1]) # removes the last enrty ("end" or "NaN" from FoCuS_scan)
        print (os.path.basename (filename), ' has been loaded')
        print ('Warning: Last row has been removed')
    elif 'csv' in filename:
        data = pd.read_csv (filename)
    else:
        print ('Error: Can\'t read file format. Please provide .xlsx or .csv')
        
    return data
        
#%% 
#Global parameters
font_size = 16
binning = 40 #Default is 40
x_min = 0
txy_max = np.log (250) # Max value on BTS  
cpm_max = 50 # Max value on BTS 
sub_sampling = 0.2 #only include 20% of the permutated data (boot-strapping) | 20% is recommended for experimental data
n_iterations = 10000 # we need a pretty large number of iterations to obtain a stable p-value. 

#%% Load data

samples = ['Control', 'Treated'] # Example conditions
data = {} # Predefine data dict
simulation = False # Indicate if data are simulated (dealing with differences in naming of variables). For analysing experimental data keep False. 
for i in range (0, len (samples)):
    data [i] =  load_sFCS_data (samples[i])
    if simulation: 
        data [i] = data[i].drop(data[i].index[-1]) 
        data[i]['txy1'] = data[i]['txy'].astype(float)
        data[i]['cpm (kHz)'] = data[i]['cpm(kHz)']
        

#%%  Prepare data
#Let's make our histograms to compare! 
data_txy1, data_cpm1 = data[0]['txy1'], data[0] ['cpm (kHz)']
data_txy2, data_cpm2 = data[1]['txy1'], data[1] ['cpm (kHz)']

fig1, axes = plt.subplots(nrows=1, ncols=2,  figsize=(12, 4)) 

for i, ax in enumerate(axes.flatten()):
    ax.set_xlabel ('Ln (Transit time (ms)/(ms))', fontsize = 14)
    ax.set_ylabel ('Brightness (kHz)', fontsize = 14)
    ax.tick_params (axis='x', labelsize = 14)
    ax.tick_params (axis='y', labelsize = 14)

histo1 = axes [0].hist2d (np.log(data_txy1), data_cpm1, bins = binning, range = [[x_min, txy_max],[0, cpm_max]], density = True) 
histo2 = axes [1].hist2d (np.log(data_txy2), data_cpm2, bins = binning, range = [[x_min, txy_max],[0, cpm_max]], density = True) 
plt.tight_layout()

#%% Now let"s use the Bhattacharyya distance and resampling to get a p-values

# we need to add a small constant to make it stable (padding)
hist1, hist2 = pad_histograms (histo1, histo2, pad = 1e-9)
# Calc hattacharyya distance
our_BD = compute_test_statistic(hist1, hist2)
#Generate Null statistics to compare to
null_statistics = generate_null_statistics (data_txy1, data_cpm1, data_txy2, data_cpm2, sub_sampling = sub_sampling, n_iterations = n_iterations)
#Calculate p-value
p_value = (np.sum(null_statistics >= our_BD) + 1) / (n_iterations + 1)
print ('Battacharyya distance p-value: ', p_value)



























    